{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9050522,"sourceType":"datasetVersion","datasetId":5456848},{"sourceId":9050574,"sourceType":"datasetVersion","datasetId":5456890},{"sourceId":9062150,"sourceType":"datasetVersion","datasetId":5465039},{"sourceId":4298,"sourceType":"modelInstanceVersion","modelInstanceId":3093,"modelId":735}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"papermill":{"default_parameters":{},"duration":2553.491047,"end_time":"2023-11-28T16:17:02.082371","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-11-28T15:34:28.591324","version":"2.4.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"036741692f3b4e0e8dcca19fa98f3d9e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4903d8aeb3294f1c95d5d1d125c108dd","placeholder":"​","style":"IPY_MODEL_e66bda58a8fa4f099c261fa2c36abf0b","value":"Extracting data files: 100%"}},"0495e2e5e9184924aa4d155b3dddcbbb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_46b3ea2401fb4aa984a0d10e7a5ec068","placeholder":"​","style":"IPY_MODEL_110dacaac11e48c18b9027d240eadcef","value":"Downloading data files: 100%"}},"06f91c1adbfd47c5857f3d26503deaae":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2582e2f3127c418388a5c174418a395a","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6d235191092b4cc3a3d6eeecf2931e8f","value":1}},"09d0a98425fe47a1be947e8534f0ff9d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0495e2e5e9184924aa4d155b3dddcbbb","IPY_MODEL_06f91c1adbfd47c5857f3d26503deaae","IPY_MODEL_8098b95438d946d4a148fb03c145b562"],"layout":"IPY_MODEL_aab68387a43d4b2c93160ca6e789c812"}},"110dacaac11e48c18b9027d240eadcef":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"185dc16921ea4229948664cf8fe051ae":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f77f1b601baa4bce850bfd99b04ca5fe","placeholder":"​","style":"IPY_MODEL_2a236cde65eb4fa49e6c841cbed9bc6a","value":"Downloading data: 100%"}},"1f85bc4f96d041e79e72e8b61052256c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"23fd00ebb04e459b9563ce0c475e78d9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_24038a5de701401e8b9d8e14d4cd4ef4","placeholder":"​","style":"IPY_MODEL_65cdec595fb540afbb887633fa828794","value":"Loading checkpoint shards: 100%"}},"24038a5de701401e8b9d8e14d4cd4ef4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2582e2f3127c418388a5c174418a395a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"266add4ec1d44b759c4f3e0c3fae3292":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2a236cde65eb4fa49e6c841cbed9bc6a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"342695a722524078b6cf56f79f68afa6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"34a251b17d73450cb0080844a3444134":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f02df16fcf774c3982a31f73ad24128d","placeholder":"​","style":"IPY_MODEL_d0a86752d95e434eb9bc8470ca23740d","value":" 1/1 [00:00&lt;00:00,  1.16ba/s]"}},"39899c8aacb941e19f806e4c93f2df25":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3c1fb4bd999a485195ccf04a80dcd5c6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4112ff7c1bc84755b9cb805393325be3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"444665d8b03748bab95d5e39ee0e7aed":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"46b3ea2401fb4aa984a0d10e7a5ec068":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4903d8aeb3294f1c95d5d1d125c108dd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4fca532232234bdcb3f4334dfdfa3bdd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"524e483d081d4787a141aafe5be44f0b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c55e2850f37e4ef7854f3fb8fd7056e7","placeholder":"​","style":"IPY_MODEL_f9e30ab3d5b14dabbfb8384a0553ab19","value":" 2/2 [00:05&lt;00:00,  2.68s/it]"}},"557e48a63e564c70a90a7e35d2178163":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d752faaa59ce4fe99d3f0e5b4cfbfbe7","placeholder":"​","style":"IPY_MODEL_d2b3cf3c19954cba9d6ce1b326a8876d","value":" 967k/967k [00:00&lt;00:00, 4.68MB/s]"}},"5a37defa3e754e25b22fafca00c331ef":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_266add4ec1d44b759c4f3e0c3fae3292","placeholder":"​","style":"IPY_MODEL_ee310cdcbd4c4bc9b20bc66348cfa39c","value":"100%"}},"5d14d8974d214138907df3c4467a3024":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_797b45db44fd4022b38d75113eda9540","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f113640d49bb4a0fbdab86bd0425263f","value":2}},"62e3d32a6ce749ea9765fcf3b250c691":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6549dbf787554b269e6a4cc7429943ce":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"657ed8e808924375a59e279e41fefafc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b505376e53ee4c0282a59e0cbdee7105","IPY_MODEL_a72b02e42cf149d397d5a645751aa74f","IPY_MODEL_c7d804a4896e437ea4a9cd9a07f53367"],"layout":"IPY_MODEL_6549dbf787554b269e6a4cc7429943ce"}},"65cdec595fb540afbb887633fa828794":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"68240f2f4161490e9ba901c7303dd7af":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6bc126bafa564933b08e22adac177244":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_23fd00ebb04e459b9563ce0c475e78d9","IPY_MODEL_5d14d8974d214138907df3c4467a3024","IPY_MODEL_524e483d081d4787a141aafe5be44f0b"],"layout":"IPY_MODEL_b071b2929815431293bebacbcc8f9e06"}},"6d235191092b4cc3a3d6eeecf2931e8f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"71ef393a797c483a84f71f0119b7d9ac":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_75ee8de1cdb64f2db9374b261e041f0a","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_812c1ff0d545455d804f0f42257114ca","value":1}},"75ee8de1cdb64f2db9374b261e041f0a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"785002d077714784b13dea870974cf68":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"797b45db44fd4022b38d75113eda9540":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8098b95438d946d4a148fb03c145b562":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f588d16e882d4b5ebcc65066cd4d4793","placeholder":"​","style":"IPY_MODEL_e6a2b0ebe51e4451af1de3475e113558","value":" 1/1 [00:00&lt;00:00,  1.73it/s]"}},"812c1ff0d545455d804f0f42257114ca":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8edcd3e3b06f447a9764446fa8f15cdf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_036741692f3b4e0e8dcca19fa98f3d9e","IPY_MODEL_71ef393a797c483a84f71f0119b7d9ac","IPY_MODEL_c5877222f4e14b3baaad7dbe4a48cfe7"],"layout":"IPY_MODEL_b8d142fe337040bcabcdeca71a6a9930"}},"a2dd6409205d4dbb81237a0e191b38ae":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_dc769762673a47b58868c82be159d033","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_62e3d32a6ce749ea9765fcf3b250c691","value":1}},"a72b02e42cf149d397d5a645751aa74f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_68240f2f4161490e9ba901c7303dd7af","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_444665d8b03748bab95d5e39ee0e7aed","value":2}},"a7584269cf4840dcb30493a944e23e81":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4112ff7c1bc84755b9cb805393325be3","max":966693,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d0cacaaea95b404c85a59c533a2357c0","value":966693}},"a78d5404f03e41599afe124da72fcf85":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5a37defa3e754e25b22fafca00c331ef","IPY_MODEL_a2dd6409205d4dbb81237a0e191b38ae","IPY_MODEL_34a251b17d73450cb0080844a3444134"],"layout":"IPY_MODEL_3c1fb4bd999a485195ccf04a80dcd5c6"}},"aab68387a43d4b2c93160ca6e789c812":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b071b2929815431293bebacbcc8f9e06":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b505376e53ee4c0282a59e0cbdee7105":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_de3d2020d24546bbaf076376887c3ea0","placeholder":"​","style":"IPY_MODEL_4fca532232234bdcb3f4334dfdfa3bdd","value":"Loading checkpoint shards: 100%"}},"b8d142fe337040bcabcdeca71a6a9930":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c3c8dc496338445989afceb3d884c802":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_185dc16921ea4229948664cf8fe051ae","IPY_MODEL_a7584269cf4840dcb30493a944e23e81","IPY_MODEL_557e48a63e564c70a90a7e35d2178163"],"layout":"IPY_MODEL_fadf4f71c9a542299c0b09b93df9a950"}},"c55e2850f37e4ef7854f3fb8fd7056e7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c5877222f4e14b3baaad7dbe4a48cfe7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_785002d077714784b13dea870974cf68","placeholder":"​","style":"IPY_MODEL_39899c8aacb941e19f806e4c93f2df25","value":" 1/1 [00:00&lt;00:00, 71.86it/s]"}},"c7d804a4896e437ea4a9cd9a07f53367":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1f85bc4f96d041e79e72e8b61052256c","placeholder":"​","style":"IPY_MODEL_342695a722524078b6cf56f79f68afa6","value":" 2/2 [03:56&lt;00:00, 108.95s/it]"}},"d0a86752d95e434eb9bc8470ca23740d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d0cacaaea95b404c85a59c533a2357c0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d2b3cf3c19954cba9d6ce1b326a8876d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d752faaa59ce4fe99d3f0e5b4cfbfbe7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dc769762673a47b58868c82be159d033":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"de3d2020d24546bbaf076376887c3ea0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e66bda58a8fa4f099c261fa2c36abf0b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e6a2b0ebe51e4451af1de3475e113558":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ee310cdcbd4c4bc9b20bc66348cfa39c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f02df16fcf774c3982a31f73ad24128d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f113640d49bb4a0fbdab86bd0425263f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f588d16e882d4b5ebcc65066cd4d4793":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f77f1b601baa4bce850bfd99b04ca5fe":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f9e30ab3d5b14dabbfb8384a0553ab19":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fadf4f71c9a542299c0b09b93df9a950":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}},"version_major":2,"version_minor":0}}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Efficient fine tuning llama2 using LoRA","metadata":{"papermill":{"duration":0.012749,"end_time":"2023-11-28T15:34:33.370063","exception":false,"start_time":"2023-11-28T15:34:33.357314","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## What is fine tuning?\nProcess of adjusting weights and parameters of a pre-trained model on new data to optimize its performance on a specific task\n\n## Requirements\n- Pretrained language model\n- New dataset specific to the task at hand\n- GPU","metadata":{"papermill":{"duration":0.012293,"end_time":"2023-11-28T15:34:33.394506","exception":false,"start_time":"2023-11-28T15:34:33.382213","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## Kaggle hardware specs","metadata":{"papermill":{"duration":0.011502,"end_time":"2023-11-28T15:34:33.417955","exception":false,"start_time":"2023-11-28T15:34:33.406453","status":"completed"},"tags":[]}},{"cell_type":"code","source":"!nvidia-smi\n!lscpu | head -n 15","metadata":{"papermill":{"duration":2.091373,"end_time":"2023-11-28T15:34:35.521025","exception":false,"start_time":"2023-11-28T15:34:33.429652","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-07-30T07:50:15.904117Z","iopub.execute_input":"2024-07-30T07:50:15.904482Z","iopub.status.idle":"2024-07-30T07:50:18.082787Z","shell.execute_reply.started":"2024-07-30T07:50:15.904451Z","shell.execute_reply":"2024-07-30T07:50:18.081295Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Tue Jul 30 07:50:16 2024       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   54C    P8              9W /   70W |       1MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   54C    P8             10W /   70W |       1MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nByte Order:                           Little Endian\nAddress sizes:                        46 bits physical, 48 bits virtual\nCPU(s):                               4\nOn-line CPU(s) list:                  0-3\nThread(s) per core:                   2\nCore(s) per socket:                   2\nSocket(s):                            1\nNUMA node(s):                         1\nVendor ID:                            GenuineIntel\nCPU family:                           6\nModel:                                85\nModel name:                           Intel(R) Xeon(R) CPU @ 2.00GHz\nStepping:                             3\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Install required libraries","metadata":{"papermill":{"duration":0.013087,"end_time":"2023-11-28T15:34:35.547409","exception":false,"start_time":"2023-11-28T15:34:35.534322","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":" - bitsandbytes - Provides 8-bit CUDA fuctions for PyTorch\n - peft - Parameter efficient fine tuning for huggingface\n - trl - Transformer reinforcement learning\n - accelerate - Huggingface integration for multiple GPUs\n - torchkeras - Enhanced PyTorch functionality\n - langchain - Framework for LLM development","metadata":{"papermill":{"duration":0.011505,"end_time":"2023-11-28T15:34:35.571940","exception":false,"start_time":"2023-11-28T15:34:35.560435","status":"completed"},"tags":[]}},{"cell_type":"code","source":"!pip install --force-reinstall --pre torch --index-url https://download.pytorch.org/whl/nightly/cu117\n","metadata":{"execution":{"iopub.status.busy":"2024-07-30T07:50:18.085383Z","iopub.execute_input":"2024-07-30T07:50:18.085927Z","iopub.status.idle":"2024-07-30T07:52:30.047663Z","shell.execute_reply.started":"2024-07-30T07:50:18.085884Z","shell.execute_reply":"2024-07-30T07:52:30.046438Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Looking in indexes: https://download.pytorch.org/whl/nightly/cu117\nCollecting torch\n  Downloading https://download.pytorch.org/whl/nightly/cu117/torch-2.1.0.dev20230621%2Bcu117-cp310-cp310-linux_x86_64.whl (1886.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 GB\u001b[0m \u001b[31m436.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting filelock (from torch)\n  Downloading https://download.pytorch.org/whl/nightly/filelock-3.13.1-py3-none-any.whl (11 kB)\nCollecting typing-extensions (from torch)\n  Downloading https://download.pytorch.org/whl/nightly/typing_extensions-4.12.2-py3-none-any.whl (37 kB)\nCollecting sympy (from torch)\n  Downloading https://download.pytorch.org/whl/nightly/sympy-1.13.1-py3-none-any.whl (6.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m87.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting networkx (from torch)\n  Downloading https://download.pytorch.org/whl/nightly/networkx-3.3-py3-none-any.whl (1.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting jinja2 (from torch)\n  Downloading https://download.pytorch.org/whl/nightly/Jinja2-3.1.4-py3-none-any.whl (133 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.3/133.3 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting fsspec (from torch)\n  Downloading https://download.pytorch.org/whl/nightly/fsspec-2024.6.1-py3-none-any.whl (177 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.6/177.6 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting pytorch-triton==2.1.0+440fd1bf20 (from torch)\n  Downloading https://download.pytorch.org/whl/nightly/pytorch_triton-2.1.0%2B440fd1bf20-cp310-cp310-linux_x86_64.whl (93.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting MarkupSafe>=2.0 (from jinja2->torch)\n  Downloading https://download.pytorch.org/whl/nightly/MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\nCollecting mpmath<1.4,>=1.1.0 (from sympy->torch)\n  Downloading https://download.pytorch.org/whl/nightly/mpmath-1.3.0-py3-none-any.whl (536 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: mpmath, typing-extensions, sympy, networkx, MarkupSafe, fsspec, filelock, pytorch-triton, jinja2, torch\n  Attempting uninstall: mpmath\n    Found existing installation: mpmath 1.3.0\n    Uninstalling mpmath-1.3.0:\n      Successfully uninstalled mpmath-1.3.0\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing_extensions 4.9.0\n    Uninstalling typing_extensions-4.9.0:\n      Successfully uninstalled typing_extensions-4.9.0\n  Attempting uninstall: sympy\n    Found existing installation: sympy 1.13.0\n    Uninstalling sympy-1.13.0:\n      Successfully uninstalled sympy-1.13.0\n  Attempting uninstall: networkx\n    Found existing installation: networkx 3.2.1\n    Uninstalling networkx-3.2.1:\n      Successfully uninstalled networkx-3.2.1\n  Attempting uninstall: MarkupSafe\n    Found existing installation: MarkupSafe 2.1.3\n    Uninstalling MarkupSafe-2.1.3:\n      Successfully uninstalled MarkupSafe-2.1.3\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2024.5.0\n    Uninstalling fsspec-2024.5.0:\n      Successfully uninstalled fsspec-2024.5.0\n  Attempting uninstall: filelock\n    Found existing installation: filelock 3.13.1\n    Uninstalling filelock-3.13.1:\n      Successfully uninstalled filelock-3.13.1\n  Attempting uninstall: jinja2\n    Found existing installation: Jinja2 3.1.2\n    Uninstalling Jinja2-3.1.2:\n      Successfully uninstalled Jinja2-3.1.2\n  Attempting uninstall: torch\n    Found existing installation: torch 2.1.2\n    Uninstalling torch-2.1.2:\n      Successfully uninstalled torch-2.1.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 24.6.1 requires cubinlinker, which is not installed.\ncudf 24.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.6.1 requires ptxcompiler, which is not installed.\ncuml 24.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 24.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 16.1.0 which is incompatible.\nbeatrix-jupyterlab 2023.128.151533 requires jupyterlab~=3.6.0, but you have jupyterlab 4.2.3 which is incompatible.\ncudf 24.6.1 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.5.0 which is incompatible.\ndatasets 2.20.0 requires fsspec[http]<=2024.5.0,>=2023.1.0, but you have fsspec 2024.6.1 which is incompatible.\ndistributed 2024.5.1 requires dask==2024.5.1, but you have dask 2024.7.0 which is incompatible.\ngcsfs 2024.5.0 requires fsspec==2024.5.0, but you have fsspec 2024.6.1 which is incompatible.\njupyterlab 4.2.3 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nmomepy 0.7.2 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.9.3 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\nrapids-dask-dependency 24.6.0a0 requires dask==2024.5.1, but you have dask 2024.7.0 which is incompatible.\ns3fs 2024.5.0 requires fsspec==2024.5.0.*, but you have fsspec 2024.6.1 which is incompatible.\nspopt 0.6.1 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.4.1 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed MarkupSafe-2.1.5 filelock-3.13.1 fsspec-2024.6.1 jinja2-3.1.4 mpmath-1.3.0 networkx-3.3 pytorch-triton-2.1.0+440fd1bf20 sympy-1.13.1 torch-2.1.0.dev20230621+cu117 typing-extensions-4.12.2\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install peft trl accelerate torchkeras langchain","metadata":{"execution":{"iopub.status.busy":"2024-07-30T07:52:30.049204Z","iopub.execute_input":"2024-07-30T07:52:30.050082Z","iopub.status.idle":"2024-07-30T07:52:53.435399Z","shell.execute_reply.started":"2024-07-30T07:52:30.050051Z","shell.execute_reply":"2024-07-30T07:52:53.434135Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting peft\n  Downloading peft-0.12.0-py3-none-any.whl.metadata (13 kB)\nCollecting trl\n  Downloading trl-0.9.6-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.32.1)\nCollecting torchkeras\n  Downloading torchkeras-3.9.9-py3-none-any.whl.metadata (8.8 kB)\nCollecting langchain\n  Downloading langchain-0.2.11-py3-none-any.whl.metadata (7.1 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.1)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.1.0.dev20230621+cu117)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.42.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.4)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.3)\nRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.23.4)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from trl) (2.20.0)\nCollecting tyro>=0.5.11 (from trl)\n  Downloading tyro-0.8.5-py3-none-any.whl.metadata (8.2 kB)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.0.25)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (3.9.1)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (4.0.3)\nCollecting langchain-core<0.3.0,>=0.2.23 (from langchain)\n  Downloading langchain_core-0.2.24-py3-none-any.whl.metadata (6.2 kB)\nCollecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n  Downloading langchain_text_splitters-0.2.2-py3-none-any.whl.metadata (2.1 kB)\nCollecting langsmith<0.2.0,>=0.1.17 (from langchain)\n  Downloading langsmith-0.1.93-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.5.3)\nRequirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.32.3)\nRequirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (8.2.3)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2024.6.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.23->langchain) (1.33)\nCollecting packaging>=20.0 (from peft)\n  Downloading packaging-24.1-py3-none-any.whl.metadata (3.2 kB)\nCollecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n  Downloading orjson-3.10.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m995.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (2.14.6)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2024.7.4)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.4)\nRequirement already satisfied: pytorch-triton==2.1.0+440fd1bf20 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (2.1.0+440fd1bf20)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2023.12.25)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.19.1)\nCollecting docstring-parser>=0.16 (from tyro>=0.5.11->trl)\n  Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\nRequirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl) (13.7.0)\nCollecting shtab>=1.5.6 (from tyro>=0.5.11->trl)\n  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (16.1.0)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (2.2.2)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (0.70.16)\nCollecting fsspec>=2023.5.0 (from huggingface-hub>=0.17.0->peft)\n  Downloading fsspec-2024.5.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.23->langchain) (2.4)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.17.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl) (2023.4)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl) (0.1.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->trl) (1.16.0)\nDownloading peft-0.12.0-py3-none-any.whl (296 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.4/296.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading trl-0.9.6-py3-none-any.whl (245 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.8/245.8 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading torchkeras-3.9.9-py3-none-any.whl (6.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading langchain-0.2.11-py3-none-any.whl (990 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m990.3/990.3 kB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_core-0.2.24-py3-none-any.whl (377 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.3/377.3 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_text_splitters-0.2.2-py3-none-any.whl (25 kB)\nDownloading langsmith-0.1.93-py3-none-any.whl (139 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading packaging-24.1-py3-none-any.whl (53 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tyro-0.8.5-py3-none-any.whl (103 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.4/103.4 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading docstring_parser-0.16-py3-none-any.whl (36 kB)\nDownloading fsspec-2024.5.0-py3-none-any.whl (316 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.1/316.1 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading orjson-3.10.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\nInstalling collected packages: torchkeras, shtab, packaging, orjson, fsspec, docstring-parser, tyro, langsmith, langchain-core, trl, peft, langchain-text-splitters, langchain\n  Attempting uninstall: packaging\n    Found existing installation: packaging 21.3\n    Uninstalling packaging-21.3:\n      Successfully uninstalled packaging-21.3\n  Attempting uninstall: orjson\n    Found existing installation: orjson 3.9.10\n    Uninstalling orjson-3.9.10:\n      Successfully uninstalled orjson-3.9.10\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2024.6.1\n    Uninstalling fsspec-2024.6.1:\n      Successfully uninstalled fsspec-2024.6.1\n  Attempting uninstall: docstring-parser\n    Found existing installation: docstring-parser 0.15\n    Uninstalling docstring-parser-0.15:\n      Successfully uninstalled docstring-parser-0.15\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 24.6.1 requires cubinlinker, which is not installed.\ncudf 24.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.6.1 requires ptxcompiler, which is not installed.\ncuml 24.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 24.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\nkeras-cv 0.9.0 requires keras-core, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 16.1.0 which is incompatible.\ncudf 24.6.1 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.5.0 which is incompatible.\ndistributed 2024.5.1 requires dask==2024.5.1, but you have dask 2024.7.0 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 24.1 which is incompatible.\njupyterlab 4.2.3 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.2 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.9.3 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\npointpats 2.5.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nrapids-dask-dependency 24.6.0a0 requires dask==2024.5.1, but you have dask 2024.7.0 which is incompatible.\nspaghetti 1.7.6 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nspopt 0.6.1 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.4.1 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed docstring-parser-0.16 fsspec-2024.5.0 langchain-0.2.11 langchain-core-0.2.24 langchain-text-splitters-0.2.2 langsmith-0.1.93 orjson-3.10.6 packaging-24.1 peft-0.12.0 shtab-1.7.1 torchkeras-3.9.9 trl-0.9.6 tyro-0.8.5\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install -U bitsandbytes","metadata":{"execution":{"iopub.status.busy":"2024-07-30T07:52:53.438501Z","iopub.execute_input":"2024-07-30T07:52:53.438959Z","iopub.status.idle":"2024-07-30T07:53:11.267268Z","shell.execute_reply.started":"2024-07-30T07:52:53.438915Z","shell.execute_reply":"2024-07-30T07:53:11.265841Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Collecting bitsandbytes\n  Downloading bitsandbytes-0.43.2-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.1.0.dev20230621+cu117)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.13.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.5.0)\nRequirement already satisfied: pytorch-triton==2.1.0+440fd1bf20 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (2.1.0+440fd1bf20)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\nDownloading bitsandbytes-0.43.2-py3-none-manylinux_2_24_x86_64.whl (137.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.5/137.5 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.43.2\n","output_type":"stream"}]},{"cell_type":"code","source":"!nvcc --version","metadata":{"execution":{"iopub.status.busy":"2024-07-30T07:53:11.269054Z","iopub.execute_input":"2024-07-30T07:53:11.269472Z","iopub.status.idle":"2024-07-30T07:53:12.298400Z","shell.execute_reply.started":"2024-07-30T07:53:11.269430Z","shell.execute_reply":"2024-07-30T07:53:12.297492Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"nvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2023 NVIDIA Corporation\nBuilt on Mon_Apr__3_17:16:06_PDT_2023\nCuda compilation tools, release 12.1, V12.1.105\nBuild cuda_12.1.r12.1/compiler.32688072_0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Imports","metadata":{"papermill":{"duration":0.012975,"end_time":"2023-11-28T15:35:20.736407","exception":false,"start_time":"2023-11-28T15:35:20.723432","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nfrom datasets import load_dataset\nimport torch\nfrom torch import nn \nfrom torch.utils.data import DataLoader \n\nfrom time import time\nimport warnings \nwarnings.filterwarnings('ignore')\n\n# Import specific libraries for acceleration and model performance monitoring\nimport accelerate \nimport peft \n\n# Import necessary modules from the transformers library\nfrom transformers import AutoTokenizer, AutoConfig, AutoModel, BitsAndBytesConfig, AutoModelForCausalLM\nfrom transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n\n# Set environment variables for PyTorch CUDA and XLA\n# max_split_size_mb - prevents allocator from splitting blocks larger than this size\nimport os\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:64\" \nos.environ['XLA_USE_BF16'] = \"1\"\nos.environ['XLA_TENSOR_ALLOCATOR_MAXSIZE'] = '100000000'\n\n# Import the torchkeras library for enhanced PyTorch functionality\nimport torchkeras","metadata":{"papermill":{"duration":27.034137,"end_time":"2023-11-28T15:35:47.783599","exception":false,"start_time":"2023-11-28T15:35:20.749462","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-07-30T07:53:12.299847Z","iopub.execute_input":"2024-07-30T07:53:12.300140Z","iopub.status.idle":"2024-07-30T07:53:24.418208Z","shell.execute_reply.started":"2024-07-30T07:53:12.300111Z","shell.execute_reply":"2024-07-30T07:53:24.417466Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"2024-07-30 07:53:14.681316: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-30 07:53:14.681422: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-30 07:53:14.805565: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Set model, dataset, output model\n","metadata":{"papermill":{"duration":0.012385,"end_time":"2023-11-28T15:35:47.809288","exception":false,"start_time":"2023-11-28T15:35:47.796903","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Model\nbase_model = \"/kaggle/input/llama-2/pytorch/7b-chat-hf/1/\"\n\n# New instruction dataset\nhrcombo_dataset = \"/kaggle/input/smallest/small_processed_data.jsonl\"\n\n# Fine-tuned model\nnew_model = \"llama-2-7b-chat-hrcombo2\"","metadata":{"papermill":{"duration":0.020641,"end_time":"2023-11-28T15:35:47.842286","exception":false,"start_time":"2023-11-28T15:35:47.821645","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-07-30T07:53:24.419244Z","iopub.execute_input":"2024-07-30T07:53:24.419776Z","iopub.status.idle":"2024-07-30T07:53:24.424285Z","shell.execute_reply.started":"2024-07-30T07:53:24.419751Z","shell.execute_reply":"2024-07-30T07:53:24.423285Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# Load dataset\n\n- Load tokenizer and set end of sentence token","metadata":{"papermill":{"duration":0.011385,"end_time":"2023-11-28T15:35:47.865616","exception":false,"start_time":"2023-11-28T15:35:47.854231","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Load and prepare data\nimport json\nfrom datasets import Dataset\nfrom datasets import load_dataset\nfrom transformers import LlamaTokenizer\n\n# Load the dataset\ntrain_dataset = load_dataset('json', data_files={\"train\":[hrcombo_dataset]}, split='train')\n\n\ndef formatInputJSONData(datapoint):\n    job_desc = \" \".join([f\"{k}: {v}\" for k, v in datapoint['job'].items()])\n    resume = \" \".join([f\"{k}: {v}\" for k, v in datapoint['resume'].items()])\n    indicators = \" \".join([f\"{k}: {v}\" for k, v in datapoint['indicators'].items()])\n    text = f\"Job Description: {job_desc}\\n\\nResume: {resume}\\n\\nIndicators: {indicators}\"\n    return text\n\n## Tokenize data\n\ntokenizer = AutoTokenizer.from_pretrained(base_model, use_fast=False,\n   trust_remote_code=True, add_eos_token=True)\n\nif tokenizer.pad_token is None:\n    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n\n# set the pad token to indicate that it's the end-of-sentence\ntokenizer.pad_token = tokenizer.eos_token\n\n## Tokenize prompt\n\ndef tokenizePrompt(prompt):\n    currPrompt = formatInputJSONData(prompt)\n    return tokenizer(currPrompt)\n\ntokenized_train_dataset = train_dataset.map(tokenizePrompt)\nprint(len(tokenized_train_dataset))\n","metadata":{"papermill":{"duration":1.276834,"end_time":"2023-11-28T15:35:49.154214","exception":false,"start_time":"2023-11-28T15:35:47.877380","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-07-30T07:53:24.425617Z","iopub.execute_input":"2024-07-30T07:53:24.425958Z","iopub.status.idle":"2024-07-30T07:53:25.542243Z","shell.execute_reply.started":"2024-07-30T07:53:24.425925Z","shell.execute_reply":"2024-07-30T07:53:25.541377Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea4e255052c545a1979ff91532908336"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/30 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c4ceda8a0c4407ca1b11c31b4f71dd7"}},"metadata":{}},{"name":"stdout","text":"30\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## **PEFT Fine-Tuning**\n\nWe now prepare the model for knowledge distillation training using the PEFT (Parameter-Efficient Fine-Tuning) method to significantly reduce the memory and compute requirements.","metadata":{}},{"cell_type":"markdown","source":"# Load model, bnbconfig\n- Set bitsandbytes config to load model with 4-bit quantization\n- Load model with bnb config","metadata":{}},{"cell_type":"code","source":"model_name_or_path = base_model\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    lm_int8_has_fp16_weight=False\n)\n\nmodel =  AutoModelForCausalLM.from_pretrained(\n    model_name_or_path,\n    quantization_config=bnb_config,\n    trust_remote_code=True,\n    low_cpu_mem_usage=True,\n    device_map = 'auto')  \nmodel.config.use_cache = False     # Disable cache for generation\nmodel.config.pretraining_tp = 1    # Slower but more accurate computation of linear layers","metadata":{"execution":{"iopub.status.busy":"2024-07-30T07:53:25.543583Z","iopub.execute_input":"2024-07-30T07:53:25.543926Z","iopub.status.idle":"2024-07-30T07:53:27.149767Z","shell.execute_reply.started":"2024-07-30T07:53:25.543892Z","shell.execute_reply":"2024-07-30T07:53:27.148399Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"Unused kwargs: ['lm_int8_has_fp16_weight']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","Cell \u001b[0;32mIn[9], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m model_name_or_path \u001b[38;5;241m=\u001b[39m base_model\n\u001b[1;32m      2\u001b[0m bnb_config \u001b[38;5;241m=\u001b[39m BitsAndBytesConfig(\n\u001b[1;32m      3\u001b[0m     load_in_4bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      4\u001b[0m     bnb_4bit_use_double_quant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m     lm_int8_has_fp16_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m      8\u001b[0m )\n\u001b[0;32m---> 10\u001b[0m model \u001b[38;5;241m=\u001b[39m  \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbnb_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m  \n\u001b[1;32m     16\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m     \u001b[38;5;66;03m# Disable cache for generation\u001b[39;00m\n\u001b[1;32m     17\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m    \u001b[38;5;66;03m# Slower but more accurate computation of linear layers\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:3279\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3276\u001b[0m     hf_quantizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3278\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 3279\u001b[0m     \u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\n\u001b[1;32m   3281\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3282\u001b[0m     torch_dtype \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_torch_dtype(torch_dtype)\n\u001b[1;32m   3283\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_device_map(device_map)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/quantizers/quantizer_bnb_4bit.py:66\u001b[0m, in \u001b[0;36mBnb4BitHfQuantizer.validate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `bitsandbytes` 4-bit quantization requires Accelerate: `pip install accelerate`\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_bitsandbytes_available():\n\u001b[0;32m---> 66\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     67\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     68\u001b[0m     )\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_tf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_flax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     72\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConverting into 4-bit or 8-bit weights from tf/flax weights is currently not supported, please make\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     73\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m sure the weights are in PyTorch format.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     74\u001b[0m     )\n","\u001b[0;31mImportError\u001b[0m: Using `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`"],"ename":"ImportError","evalue":"Using `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`","output_type":"error"}]},{"cell_type":"code","source":"from peft import prepare_model_for_kbit_training\nfrom peft import LoraConfig, get_peft_model\n\n# gradient checkpointing to reduce memory usage for increased compute time\nmodel.gradient_checkpointing_enable()\n\n# compressing the base model into a smaller, more efficient model\nmodel = prepare_model_for_kbit_training(model)","metadata":{"execution":{"iopub.status.busy":"2024-07-30T07:38:03.459616Z","iopub.status.idle":"2024-07-30T07:38:03.460001Z","shell.execute_reply.started":"2024-07-30T07:38:03.459830Z","shell.execute_reply":"2024-07-30T07:38:03.459845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Configure model with LoRA**\n\nThe code below uses LoRA (a PEFT method) to reduce the number of trainable parameters. LoRA works by decomposing the large matrix of the pre-trained model into two smaller low-rank matrices in the attention layers which drastically reduces the number of parameters that need to be fine-tuned. ","metadata":{}},{"cell_type":"code","source":"config = LoraConfig(\n    r=8,# rank of the update matrices,Lower rank results in smaller matrices with fewer trainable params\n    lora_alpha=64,# impacts low-rank approximation aggressiveness,increasing value speeds up training\n    target_modules=[\n        \"q_proj\",\n        \"k_proj\",\n        \"v_proj\",\n        \"gate_proj\",\n        \"down_proj\",\n        \"up_proj\",\n        \"o_proj\"\n    ], # modules to apply the LoRA update matrices\n    bias=\"none\", # determines LoRA bias type, influencing training dynamics\n    lora_dropout=0.05, # regulates model regularization; increasing may lead to underfitting\n    task_type=\"CAUSAL_LM\",\n)\n\nmodel = get_peft_model(model, config)","metadata":{"execution":{"iopub.status.busy":"2024-07-30T07:38:03.462022Z","iopub.status.idle":"2024-07-30T07:38:03.462376Z","shell.execute_reply.started":"2024-07-30T07:38:03.462209Z","shell.execute_reply":"2024-07-30T07:38:03.462224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Training the model**\n\nWe're now ready to train our Llama 2 model on our new data! We'll be using the Transformers library to create a Trainer object for training the model. The Trainer takes the pre-trained model, training datasets, training arguments, and data collator as input.\n\nTraining time depends on the size of the training data, number of epochs and the configuration of the GPU used. ","metadata":{}},{"cell_type":"code","source":"import transformers\n\ntrainer = transformers.Trainer(\n    model=model,                             # llama-2-7b-chat model\n    train_dataset=tokenized_train_dataset,   # training data that's tokenized\n    args=transformers.TrainingArguments(\n        output_dir=\"/kaggle/working/results\",       # directory where checkpoints are saved\n        per_device_train_batch_size=2,       # number of samples processed in one forward/backward pass per GPU\n        gradient_accumulation_steps=2,       # [default = 1] number of updates steps to accumulate the gradients for\n        max_steps = 100,   \n        learning_rate=1e-4,                  # [IMPORTANT] smaller LR for better finetuning\n        bf16=False,                          # train parameters with this precision\n        optim=\"paged_adamw_8bit\",            # use paging to improve memory management of default adamw optimizer\n        logging_dir=\"/kaggle/working/logs\",                # directory to save training log outputs\n        save_strategy=\"steps\",               # [default = \"steps\"] store after every iteration of a datapoint\n        save_steps=10,                       # save checkpoint after number of iterations\n        logging_steps = 10,                   # specify frequency of printing training loss data\n        run_name=\"llama-2-hrcombo\"\n    ),\n\n    # use to form a batch from a list of elements of train_dataset\n    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n)\n\n# if use_cache is True, past key values are used to speed up decoding\n# if applicable to model. This defeats the purpose of finetuning\nmodel.config.use_cache = False\n\n# train the model based on the above config\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-07-30T07:38:03.463477Z","iopub.status.idle":"2024-07-30T07:38:03.463828Z","shell.execute_reply.started":"2024-07-30T07:38:03.463630Z","shell.execute_reply":"2024-07-30T07:38:03.463643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.save_model(new_model)","metadata":{"execution":{"iopub.status.busy":"2024-07-30T07:38:03.465268Z","iopub.status.idle":"2024-07-30T07:38:03.465600Z","shell.execute_reply.started":"2024-07-30T07:38:03.465429Z","shell.execute_reply":"2024-07-30T07:38:03.465442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!zip -r file.zip /kaggle/working/llama-2-7b-chat-hrcombo2","metadata":{"execution":{"iopub.status.busy":"2024-07-30T05:37:40.404862Z","iopub.execute_input":"2024-07-30T05:37:40.405763Z","iopub.status.idle":"2024-07-30T05:37:47.884915Z","shell.execute_reply.started":"2024-07-30T05:37:40.405717Z","shell.execute_reply":"2024-07-30T05:37:47.883967Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"  adding: kaggle/working/llama-2-7b-chat-hrcombo/ (stored 0%)\n  adding: kaggle/working/llama-2-7b-chat-hrcombo/adapter_model.safetensors (deflated 7%)\n  adding: kaggle/working/llama-2-7b-chat-hrcombo/training_args.bin (deflated 51%)\n  adding: kaggle/working/llama-2-7b-chat-hrcombo/adapter_config.json (deflated 53%)\n  adding: kaggle/working/llama-2-7b-chat-hrcombo/README.md (deflated 66%)\n","output_type":"stream"}]},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'file.zip')","metadata":{"execution":{"iopub.status.busy":"2024-07-30T05:38:15.793309Z","iopub.execute_input":"2024-07-30T05:38:15.793698Z","iopub.status.idle":"2024-07-30T05:38:15.801264Z","shell.execute_reply.started":"2024-07-30T05:38:15.793669Z","shell.execute_reply":"2024-07-30T05:38:15.800315Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/file.zip","text/html":"<a href='file.zip' target='_blank'>file.zip</a><br>"},"metadata":{}}]},{"cell_type":"markdown","source":"## **Load Finetuned Model**","metadata":{}},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-07-28T13:00:39.152675Z","iopub.execute_input":"2024-07-28T13:00:39.153249Z","iopub.status.idle":"2024-07-28T13:00:39.485370Z","shell.execute_reply.started":"2024-07-28T13:00:39.153218Z","shell.execute_reply":"2024-07-28T13:00:39.483983Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom transformers import BitsAndBytesConfig,LlamaTokenizer\nfrom peft import PeftModel\n\ntime_1 = time()\nmodel_name_or_path = base_model\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    llm_int8_has_fp16_weight=False,\n)\n\ntokenizer = LlamaTokenizer.from_pretrained(model_name_or_path, use_fast=False,\n                                           trust_remote_code=True,\n                                           add_eos_token=True)\nif tokenizer.pad_token is None:\n    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n\n# set the pad token to indicate that it's the end-of-sentence\ntokenizer.pad_token = tokenizer.eos_token\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name_or_path,  #same as before\n    quantization_config=bnb_config,  #same quantization config as before\n    device_map=\"auto\",\n    trust_remote_code=True,\n    low_cpu_mem_usage=True\n)\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\n\ntokenizer = LlamaTokenizer.from_pretrained(model_name_or_path, use_fast=False,\n                                           trust_remote_code=True)\n\nmodelFinetuned = PeftModel.from_pretrained(model,\"/kaggle/working/results/checkpoint-50\")\n\ntime_2 = time()\nprint(f\"Time elapsed: {round(time_2-time_1, 3)} sec.\")","metadata":{"execution":{"iopub.status.busy":"2024-07-28T13:01:09.323707Z","iopub.execute_input":"2024-07-28T13:01:09.324382Z","iopub.status.idle":"2024-07-28T13:01:16.442657Z","shell.execute_reply.started":"2024-07-28T13:01:09.324349Z","shell.execute_reply":"2024-07-28T13:01:16.441705Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8c64cc9738a4d5ba5590f3182592225"}},"metadata":{}},{"name":"stdout","text":"Time elapsed: 7.109 sec.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Save model","metadata":{}},{"cell_type":"code","source":"# modelFinetuned.save_model('/kaggle/working/' + new_model)","metadata":{"execution":{"iopub.status.busy":"2024-07-28T10:01:16.408067Z","iopub.status.idle":"2024-07-28T10:01:16.408438Z","shell.execute_reply.started":"2024-07-28T10:01:16.408266Z","shell.execute_reply":"2024-07-28T10:01:16.408281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!zip -r file.zip /kaggle/working","metadata":{"execution":{"iopub.status.busy":"2024-07-28T10:01:16.409639Z","iopub.status.idle":"2024-07-28T10:01:16.409981Z","shell.execute_reply.started":"2024-07-28T10:01:16.409808Z","shell.execute_reply":"2024-07-28T10:01:16.409823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'file.zip')","metadata":{"execution":{"iopub.status.busy":"2024-07-28T10:01:16.411397Z","iopub.status.idle":"2024-07-28T10:01:16.411754Z","shell.execute_reply.started":"2024-07-28T10:01:16.411572Z","shell.execute_reply":"2024-07-28T10:01:16.411587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Llama 2 prompt format**\n- \\<s>, \\</s> - marks start and end of coversation\n- [INST], [/INST] - marks start and end of user prompt\n- \\<\\<SYS>>, \\<\\</SYS>> - marks start and end of system prompt","metadata":{}},{"cell_type":"code","source":"### Function to create prompts formatted for llama 2, Includes default system prompt\n\nDEFAULT_SYSTEM_PROMPT = \"\"\"\nYou are a helpful, respectful and honest human resource assistant. Always answer as helpfully as possible, while being safe. \nYour answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. \nIf you don't know the answer to a question, please don't share false information.\n\"\"\".strip()\n\n\ndef generate_prompt(prompt: str, system_prompt: str = DEFAULT_SYSTEM_PROMPT) -> str:\n    return f\"\"\"\n[INST] <<SYS>>\n{system_prompt}\n<</SYS>>\n\n{prompt} [/INST]\n\"\"\".strip()","metadata":{"execution":{"iopub.status.busy":"2024-07-30T07:22:17.359316Z","iopub.execute_input":"2024-07-30T07:22:17.360216Z","iopub.status.idle":"2024-07-30T07:22:17.367116Z","shell.execute_reply.started":"2024-07-30T07:22:17.360178Z","shell.execute_reply":"2024-07-30T07:22:17.365940Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Test inference of fine tuned model","metadata":{"papermill":{"duration":0.015485,"end_time":"2023-11-28T16:16:22.500897","exception":false,"start_time":"2023-11-28T16:16:22.485412","status":"completed"},"tags":[]}},{"cell_type":"code","source":"    sample_job = \"\"\"\n    Software Engineer\nTech Solutions Inc. is a forward-thinking technology company focused on innovative software solutions. We strive to create an inclusive and collaborative environment where creativity and technical excellence thrive.\nTo provide cutting-edge software solutions that empower businesses to achieve their goals.\nTo be the leading provider of innovative software solutions worldwide.\nInnovation Collaboration Excellence\nInclusive collaborative and innovative\nWe are seeking a highly skilled Software Engineer to join our dynamic team. The ideal candidate will have strong experience in software development a passion for technology and a drive to deliver high-quality software solutions.\nDesign develop and maintain software applications.\nCollaborate with cross-functional teams to define and implement software requirements.\nWrite clean efficient and maintainable code.\nPerform code reviews and provide constructive feedback.\nTroubleshoot and debug software issues.\nBachelor's degree in Computer Science or a related field.\nJava Python JavaScript SQL Software Development Lifecycle (SDLC)\n3+ years of experience in software development Experience with cloud platforms (e.g. AWS Azure) Knowledge of Agile methodologies Experience with containerization (e.g. Docker)\nExperience in the tech industry is preferred. Strong problem-solving skills Excellent communication skills Ability to work both independently and in a team Attention to detail\n\n    \"\"\"\n    sample_resume = \"\"\"\n    Skilled Software Engineer with over 4 years of experience in developing robust software applications. Proficient in Java, Python, and JavaScript, with a strong understanding of cloud platforms and Agile methodologies. Seeking to leverage my expertise to contribute to the success of Tech Solutions Inc.\nSoftware Engineer\nDesigned and implemented new features for a cloud-based application, improving user experience by 30%.\nCollaborated with cross-functional teams to define project requirements and deliver high-quality software solutions.\nOptimized database queries, reducing load times by 25%.\nMentored junior developers and conducted code reviews.\nJunior Software Engineer\nDeveloped and maintained web applications using JavaScript and Python.\nAssisted in migrating legacy applications to cloud infrastructure (AWS).\nTroubleshot and resolved software issues, improving application stability.\nBachelor of Science in Computer Science\nProgramming Languages: Java, Python, JavaScript,\nWeb Technologies: HTML, CSS, React,\nDatabases: MySQL, PostgreSQL,\nCloud Platforms: AWS, Azure,\nDevelopment Tools: Git, Docker,\nSoft Skills: Problem-solving, Communication, Teamwork\nAWS Certified Developer – Associate,\nE-commerce Platform Development,\nLead Developer,\nDeveloped a scalable e-commerce platform using Java and Spring Boot.\nImplemented RESTful APIs to support front-end applications.\nIntegrated third-party payment gateways and shipping services.\nJava, Spring Boot, MySQL\nIEEE Computer Society,\nVolunteer Developer,\nCode for Good,\nDeveloped and maintained software solutions for non-profit organizations.\nProvided technical support and training to volunteers.\nEnglish,\nFluent\nSpanish,\nIntermediate\n    \"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-07-30T07:22:21.870069Z","iopub.execute_input":"2024-07-30T07:22:21.870419Z","iopub.status.idle":"2024-07-30T07:22:21.877699Z","shell.execute_reply.started":"2024-07-30T07:22:21.870390Z","shell.execute_reply":"2024-07-30T07:22:21.876882Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-07-28T10:01:16.416748Z","iopub.status.idle":"2024-07-28T10:01:16.417096Z","shell.execute_reply.started":"2024-07-28T10:01:16.416912Z","shell.execute_reply":"2024-07-28T10:01:16.416926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"time_1 = time()\n\nrequest = '''\n    {sample_job}\n    {sample_resume}\n    Do not say any other thing, just return the indicators as a json.\n    '''\n\n# Format the question\neval_prompt = generate_prompt(request.format(sample_job=sample_job, sample_resume=sample_resume))\npromptTokenized = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n\nmodelFinetuned.eval()\nwith torch.no_grad():\n    print(tokenizer.decode(modelFinetuned.generate(**promptTokenized, max_new_tokens = 1024)[0], skip_special_tokens=True))\ntorch.cuda.empty_cache()\n\ntime_2 = time()\nprint(f\"Time elapsed: {round(time_2-time_1, 3)} sec.\")","metadata":{"execution":{"iopub.status.busy":"2024-07-28T10:01:16.418532Z","iopub.status.idle":"2024-07-28T10:01:16.418875Z","shell.execute_reply.started":"2024-07-28T10:01:16.418710Z","shell.execute_reply":"2024-07-28T10:01:16.418725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-07-28T10:01:16.420209Z","iopub.status.idle":"2024-07-28T10:01:16.420590Z","shell.execute_reply.started":"2024-07-28T10:01:16.420416Z","shell.execute_reply":"2024-07-28T10:01:16.420436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"time_1 = time()\n\nrequest = '''\n    Given a job description and a resume, analyse and compare them and return the results in the following format in indicators:\n    The job description is: \n    {sample_job}\n    \n    The resume is: \n    {sample_resume}\n    \n    The indicators to be assigned in the following form:\n    overallMatchPercentage: \"\"\n    culturalFitScore: \"\"\n    culturalFitScale: 5,\n    culturalFitDescription: \"\"\n    culturalFitReasoning: \"\"\n    growthPotentialScore: \"\"\n    growthPotentialScale: 10,\n    growthPotentialDescription: \"\"\n    growthPotentialReasoning: \"\"\n    productivityIndicatorScore: \"\"\n    productivityIndicatorScale: 5,\n    productivityIndicatorDescription: \"\"\n    productivityIndicatorReasoning: \"\"\n    likelihoodOfJobOfferAcceptanceScore: \"\"\n    likelihoodOfJobOfferAcceptanceScale: 10,\n    likelihoodOfJobOfferAcceptanceDescription: \"\"\n    likelihoodOfJobOfferAcceptanceReasoning: \"\"\n    predictedCandidateSuccessScore: \"\"\n    predictedCandidateSuccessScale: 10,\n    predictedCandidateSuccessDescription: \"\"\n    predictedCandidateSuccessReasoning: \"\"\n    longTermEngagementAndRetentionScore: \"\"\n    longTermEngagementAndRetentionScale: 5,\n    longTermEngagementAndRetentionDescription: \"\"\n    longTermEngagementAndRetentionReasoning: \"\"\n    skillMatchScore: \"\"\n    skillMatchScale: 10,\n    skillMatchDescription: \"\"\n    skillMatchReasoning: \"\"\n    \n    Do not say any other thing, just return the indicators as a json.\n    '''\n\n# Format the question\neval_prompt = generate_prompt(request.format(sample_job=sample_job, sample_resume=sample_resume))\npromptTokenized = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n\nmodelFinetuned.eval()\nwith torch.no_grad():\n    print(tokenizer.decode(modelFinetuned.generate(**promptTokenized, max_new_tokens = 1024)[0], skip_special_tokens=True))\ntorch.cuda.empty_cache()\n\ntime_2 = time()\nprint(f\"Time elapsed: {round(time_2-time_1, 3)} sec.\")","metadata":{"execution":{"iopub.status.busy":"2024-07-28T13:01:43.725649Z","iopub.execute_input":"2024-07-28T13:01:43.726367Z","iopub.status.idle":"2024-07-28T14:29:29.199712Z","shell.execute_reply.started":"2024-07-28T13:01:43.726336Z","shell.execute_reply":"2024-07-28T14:29:29.198108Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"[INST] <<SYS>>\nYou are a helpful, respectful and honest human resource assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n<</SYS>>\n\n\n    Given a job description and a resume, analyse and compare them and return the results in the following format in indicators:\n    The job description is: \n    \nSoftware Engineer\nTech Solutions Inc. is a forward-thinking technology company focused on innovative software solutions. We strive to create an inclusive and collaborative environment where creativity and technical excellence thrive.\nTo provide cutting-edge software solutions that empower businesses to achieve their goals.\nTo be the leading provider of innovative software solutions worldwide.\nInnovation Collaboration Excellence\nInclusive collaborative and innovative\nWe are seeking a highly skilled Software Engineer to join our dynamic team. The ideal candidate will have strong experience in software development a passion for technology and a drive to deliver high-quality software solutions.\nDesign develop and maintain software applications.\nCollaborate with cross-functional teams to define and implement software requirements.\nWrite clean efficient and maintainable code.\nPerform code reviews and provide constructive feedback.\nTroubleshoot and debug software issues.\nBachelor's degree in Computer Science or a related field.\nJava Python JavaScript SQL Software Development Lifecycle (SDLC)\n3+ years of experience in software development Experience with cloud platforms (e.g. AWS Azure) Knowledge of Agile methodologies Experience with containerization (e.g. Docker)\nExperience in the tech industry is preferred. Strong problem-solving skills Excellent communication skills Ability to work both independently and in a team Attention to detail\n\n\n    \n    The resume is: \n    \nSkilled Software Engineer with over 4 years of experience in developing robust software applications. Proficient in Java, Python, and JavaScript, with a strong understanding of cloud platforms and Agile methodologies. Seeking to leverage my expertise to contribute to the success of Tech Solutions Inc.\nSoftware Engineer\nDesigned and implemented new features for a cloud-based application, improving user experience by 30%.\nCollaborated with cross-functional teams to define project requirements and deliver high-quality software solutions.\nOptimized database queries, reducing load times by 25%.\nMentored junior developers and conducted code reviews.\nJunior Software Engineer\nDeveloped and maintained web applications using JavaScript and Python.\nAssisted in migrating legacy applications to cloud infrastructure (AWS).\nTroubleshot and resolved software issues, improving application stability.\nBachelor of Science in Computer Science\nProgramming Languages: Java, Python, JavaScript,\nWeb Technologies: HTML, CSS, React,\nDatabases: MySQL, PostgreSQL,\nCloud Platforms: AWS, Azure,\nDevelopment Tools: Git, Docker,\nSoft Skills: Problem-solving, Communication, Teamwork\nAWS Certified Developer – Associate,\nE-commerce Platform Development,\nLead Developer,\nDeveloped a scalable e-commerce platform using Java and Spring Boot.\nImplemented RESTful APIs to support front-end applications.\nIntegrated third-party payment gateways and shipping services.\nJava, Spring Boot, MySQL\nIEEE Computer Society,\nVolunteer Developer,\nCode for Good,\nDeveloped and maintained software solutions for non-profit organizations.\nProvided technical support and training to volunteers.\nEnglish,\nFluent\nSpanish,\nIntermediate\n\n    \n    The indicators to be assigned in the following form:\n    overallMatchPercentage: \"\"\n    culturalFitScore: \"\"\n    culturalFitScale: 5,\n    culturalFitDescription: \"\"\n    culturalFitReasoning: \"\"\n    growthPotentialScore: \"\"\n    growthPotentialScale: 10,\n    growthPotentialDescription: \"\"\n    growthPotentialReasoning: \"\"\n    productivityIndicatorScore: \"\"\n    productivityIndicatorScale: 5,\n    productivityIndicatorDescription: \"\"\n    productivityIndicatorReasoning: \"\"\n    likelihoodOfJobOfferAcceptanceScore: \"\"\n    likelihoodOfJobOfferAcceptanceScale: 10,\n    likelihoodOfJobOfferAcceptanceDescription: \"\"\n    likelihoodOfJobOfferAcceptanceReasoning: \"\"\n    predictedCandidateSuccessScore: \"\"\n    predictedCandidateSuccessScale: 10,\n    predictedCandidateSuccessDescription: \"\"\n    predictedCandidateSuccessReasoning: \"\"\n    longTermEngagementAndRetentionScore: \"\"\n    longTermEngagementAndRetentionScale: 5,\n    longTermEngagementAndRetentionDescription: \"\"\n    longTermEngagementAndRetentionReasoning: \"\"\n    skillMatchScore: \"\"\n    skillMatchScale: 10,\n    skillMatchDescription: \"\"\n    skillMatchReasoning: \"\"\n    \n    Do not say any other thing, just return the indicators as a json.\n     [/INST]  Sure, I can help you with that! Based on the job description and resume you provided, here are the indicators for the candidate:\n\nOverallMatchPercentage: 85.67105931 culturalFitScore: 10 culturalFitScale: 5 culturalFitDescription: Uncertain culturalFitReasoning: Due to the significant gap in relevant experience. growthPotentialScore: 10 growthPotentialScale: 10 growthPotentialDescription: Low growthPotentialReasoning: Candidate may require higher-level challenges to stay motivated. productivityIndicatorScore: 7.0 productivityIndicatorScale: 5 productivityIndicatorDescription: Moderate productivityIndicatorReasoning: Risk of the candidate leaving for more senior roles. likelihoodOfJobOfferAcceptanceScore: 10 likelihoodOfJobOfferAcceptanceScale: 10 likelihoodOfJobOfferAcceptanceDescription: High likelihoodOfJobOfferAcceptanceReasoning: Candidate's alignment with job requirements is strong. predictedCandidateSuccessScore: 10.0 predictedCandidateSuccessScale: 10 predictedCandidateSuccessDescription: Uncertain predictedCandidateSuccessReasoning: Due to the significant gap in relevant experience. longTermEngagementAndRetentionScore: 1.0 longTermEngagementAndRetentionScale: 5 longTermEngagementAndRetentionDescription: Low longTermEngagementAndRetentionReasoning: Candidate may require higher-level challenges to stay motivated. skillMatchScore: 9 skillMatchScale: 10 skillMatchDescription: Advanced skillMatchReasoning: The candidate demonstrates advanced proficiency overall, with skills in programming (advanced), software development lifecycle (proficient), cloud platforms (proficient), agile methodologies (proficient), communication (advanced), teamwork (proficient).\n\nOverall, the candidate demonstrates advanced proficiency overall, with skills in programming (advanced), software development lifecycle (proficient), cloud platforms (proficient), agile methodologies (proficient), communication (advanced), teamwork (proficient). The candidate's alignment with job requirements is strong, and they may require higher-level challenges to stay motivated. Due to the significant gap in relevant experience, the candidate's predicted success and long-term engagement and retention are uncertain.\nTime elapsed: 5265.462 sec.\n","output_type":"stream"}]},{"cell_type":"code","source":"import shutil\nimport os\n\n# Paths to the original and fine-tuned model directories\noriginal_model_dir = \"/kaggle/input/llama-2/pytorch/7b-chat-hf/1\"\nfinetuned_model_dir = \"/kaggle/working/llama-2-7b-chat-hrcombo\"\n\n# List of essential files\nessential_files = [\"config.json\", \"tokenizer_config.json\", \"vocab.json\", \"merges.txt\"]\n\nfor file_name in essential_files:\n    src_file = os.path.join(original_model_dir, file_name)\n    dst_file = os.path.join(finetuned_model_dir, file_name)\n    if not os.path.exists(dst_file) and os.path.exists(src_file):\n        shutil.copy(src_file, dst_file)\n\nprint(\"Missing files copied successfully.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-30T06:08:02.419053Z","iopub.execute_input":"2024-07-30T06:08:02.419452Z","iopub.status.idle":"2024-07-30T06:08:02.444876Z","shell.execute_reply.started":"2024-07-30T06:08:02.419422Z","shell.execute_reply":"2024-07-30T06:08:02.444018Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Missing files copied successfully.\n","output_type":"stream"}]},{"cell_type":"code","source":"model_finetuned = AutoModelForCausalLM.from_pretrained(new_model)\ntokenizer = AutoTokenizer.from_pretrained(new_model)\neval_prompt = generate_prompt(request.format(\n    sample_job=sample_job, sample_resume=sample_resume))\nprompt_tokenized = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n\nmodel_finetuned.eval()\nwith torch.no_grad():\n    result = tokenizer.decode(model_finetuned.generate(\n        **prompt_tokenized, max_new_tokens=1024)[0], skip_special_tokens=True)\n\nprint(result)\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-07-30T07:22:50.679647Z","iopub.execute_input":"2024-07-30T07:22:50.680043Z","iopub.status.idle":"2024-07-30T07:22:52.241167Z","shell.execute_reply.started":"2024-07-30T07:22:50.680014Z","shell.execute_reply":"2024-07-30T07:22:52.239779Z"},"trusted":true},"execution_count":8,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model_finetuned \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(new_model)\n\u001b[1;32m      3\u001b[0m eval_prompt \u001b[38;5;241m=\u001b[39m generate_prompt(request\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m      4\u001b[0m     sample_job\u001b[38;5;241m=\u001b[39msample_job, sample_resume\u001b[38;5;241m=\u001b[39msample_resume))\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:3382\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3377\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   3378\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError no file named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_add_variant(SAFE_WEIGHTS_NAME,\u001b[38;5;250m \u001b[39mvariant)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m found in directory\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3379\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3380\u001b[0m         )\n\u001b[1;32m   3381\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3382\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   3383\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError no file named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_add_variant(WEIGHTS_NAME,\u001b[38;5;250m \u001b[39mvariant)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_add_variant(SAFE_WEIGHTS_NAME,\u001b[38;5;250m \u001b[39mvariant)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3384\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTF2_WEIGHTS_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTF_WEIGHTS_NAME\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.index\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mFLAX_WEIGHTS_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m found in directory\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3385\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3386\u001b[0m         )\n\u001b[1;32m   3387\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(subfolder, pretrained_model_name_or_path)):\n\u001b[1;32m   3388\u001b[0m     archive_file \u001b[38;5;241m=\u001b[39m pretrained_model_name_or_path\n","\u001b[0;31mOSError\u001b[0m: Error no file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory llama-2-7b-chat-hrcombo."],"ename":"OSError","evalue":"Error no file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory llama-2-7b-chat-hrcombo.","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}